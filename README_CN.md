# Qwen2.5-Omni
<p align="left">
        ä¸­æ–‡</a>&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>
<br>

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/Omni_logo.png" width="400"/>
<p>

<p align="center">
        ğŸ’œ <a href="https://chat.qwenlm.ai/"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbspğŸ¤— <a href="https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e">Hugging Face</a>&nbsp&nbsp | &nbsp&nbspğŸ¤– <a href="https://modelscope.cn/collections/Qwen25-Omni-a2505ce0d5514e">ModelScope</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href="https://qwenlm.github.io/blog/qwen2.5-omni/">Blog</a>&nbsp&nbsp | &nbsp&nbspğŸ“š <a href="https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks">Cookbooks</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href="https://arxiv.org/abs/2503.20215">Paper</a>&nbsp&nbsp
<br>
ğŸ–¥ï¸ <a href="https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo">Demo</a>&nbsp&nbsp | &nbsp&nbspğŸ’¬ <a href="https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png">WeChat (å¾®ä¿¡)</a>&nbsp&nbsp | &nbsp&nbspğŸ«¨ <a href="https://discord.gg/CV4E9rpNSD">Discord</a>&nbsp&nbsp | &nbsp&nbspğŸ“‘ <a href="https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni">API</a>
<!-- &nbsp&nbsp | &nbsp&nbspğŸ–¥ï¸ <a href="https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl">PAI-DSW</a> -->
</p>

**Qwen2.5-Omni**æ­£å¼å‘å¸ƒäº†ï¼è¿™æ˜¯Qwenç³»åˆ—ä¸­å…¨æ–°çš„æ——èˆ°çº§ç«¯åˆ°ç«¯å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼Œä¸“ä¸ºå…¨é¢çš„å¤šæ¨¡å¼æ„ŸçŸ¥è®¾è®¡ï¼Œæ— ç¼å¤„ç†åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åœ¨å†…çš„å„ç§è¾“å…¥ï¼ŒåŒæ—¶æ”¯æŒæµå¼çš„æ–‡æœ¬ç”Ÿæˆå’Œè‡ªç„¶è¯­éŸ³åˆæˆè¾“å‡ºï¼Œè®©æˆ‘ä»¬ç‚¹å‡»ä¸‹æ–¹è§†é¢‘äº†è§£æ›´å¤šä¿¡æ¯å§ ğŸ˜ƒ

<a href="https://youtu.be/UF55yM67EH0" target="_blank">
  <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/video_cover.png" alt="Open Video"/>
</a>

## æ–°é—»
* 2025.03.29: â­ï¸â­ï¸â­ï¸ Qwen2.5-Omni è¾¾åˆ° Hugging Face Trending æ¦œçš„ top-2! 
* 2025.03.26: ä¸Qwen2.5-Omniçš„å®æ—¶äº¤äº’ä½“éªŒå·²ç»åœ¨ [Qwen Chat](https://chat.qwen.ai/) ä¸Šçº¿ï¼Œæ¬¢è¿ä½“éªŒ!
* 2025.03.26: æˆ‘ä»¬å‘å¸ƒäº† [Qwen2.5-Omni](https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e). å¯¹äºæ›´å¤šçš„ä¿¡æ¯è¯·è®¿é—®æˆ‘ä»¬çš„[åšå®¢](https://qwenlm.github.io/zh/blog/qwen2.5-omni/)!


## ç›®å½• <!-- omit in toc -->

- [æ¦‚è§ˆ](#æ¦‚è§ˆ)
  - [ç®€ä»‹](#ç®€ä»‹)
  - [ä¸»è¦ç‰¹ç‚¹](#ä¸»è¦ç‰¹ç‚¹)
  - [æ¨¡å‹ç»“æ„](#æ¨¡å‹ç»“æ„)
  - [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
  - [Transformers ä½¿ç”¨æ–¹æ³•](#--transformers-ä½¿ç”¨æ–¹æ³•)
  - [ModelScope ä½¿ç”¨æ–¹æ³•](#-modelscope-ä½¿ç”¨æ–¹æ³•)
  - [ä½¿ç”¨æç¤º](#ä½¿ç”¨æç¤º)
  - [æ›´å¤šä½¿ç”¨ç”¨ä¾‹çš„ Cookbooks](#æ›´å¤šä½¿ç”¨ç”¨ä¾‹çš„-cookbooks)
  - [API æ¨ç†](#api-æ¨ç†)
- [å’Œ Qwen2.5-Omni å¯¹è¯](#å’Œ-qwen25-omni-å¯¹è¯)
  - [åœ¨çº¿æ¼”ç¤º](#åœ¨çº¿æ¼”ç¤º)
  - [å¯åŠ¨æœ¬åœ°ç½‘é¡µæ¼”ç¤º](#å¯åŠ¨æœ¬åœ°ç½‘é¡µæ¼”ç¤º)
  - [å®æ—¶äº¤äº’](#å®æ—¶äº¤äº’)
- [ä½¿ç”¨vLLMéƒ¨ç½²](#ä½¿ç”¨vLLMéƒ¨ç½²)
- [Docker](#-docker)
<!-- - [å¼•ç”¨](#citation) -->

## æ¦‚è§ˆ
### ç®€ä»‹
Qwen 2.5-Omniæ˜¯ä¸€ä¸ªç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼Œæ—¨åœ¨æ„ŸçŸ¥åŒ…æ‹¬æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘åœ¨å†…çš„å¤šç§æ¨¡æ€ï¼ŒåŒæ—¶ä»¥æµå¼çš„æ–¹å¼ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/qwen_omni.png" width="80%"/>
<p>

### ä¸»è¦ç‰¹ç‚¹

* **å…¨èƒ½åˆ›æ–°æ¶æ„**ï¼šæˆ‘ä»¬æå‡ºäº†ä¸€ç§å…¨æ–°çš„Thinker-Talkeræ¶æ„ï¼Œè¿™æ˜¯ä¸€ç§ç«¯åˆ°ç«¯çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ—¨åœ¨æ”¯æŒæ–‡æœ¬/å›¾åƒ/éŸ³é¢‘/è§†é¢‘çš„è·¨æ¨¡æ€ç†è§£ï¼ŒåŒæ—¶ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³å“åº”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„ä½ç½®ç¼–ç æŠ€æœ¯ï¼Œç§°ä¸ºTMRoPEï¼ˆTime-aligned Multimodal RoPEï¼‰ï¼Œé€šè¿‡æ—¶é—´è½´å¯¹é½å®ç°è§†é¢‘ä¸éŸ³é¢‘è¾“å…¥çš„ç²¾å‡†åŒæ­¥ã€‚

* **å®æ—¶éŸ³è§†é¢‘äº¤äº’**ï¼šæ¶æ„æ—¨åœ¨æ”¯æŒå®Œå…¨å®æ—¶äº¤äº’ï¼Œæ”¯æŒåˆ†å—è¾“å…¥å’Œå³æ—¶è¾“å‡ºã€‚

* **è‡ªç„¶æµç•…çš„è¯­éŸ³ç”Ÿæˆ**ï¼šåœ¨è¯­éŸ³ç”Ÿæˆçš„è‡ªç„¶æ€§å’Œç¨³å®šæ€§æ–¹é¢è¶…è¶Šäº†è®¸å¤šç°æœ‰çš„æµå¼å’Œéæµå¼æ›¿ä»£æ–¹æ¡ˆã€‚

* **å…¨æ¨¡æ€æ€§èƒ½ä¼˜åŠ¿**ï¼šåœ¨åŒç­‰è§„æ¨¡çš„å•æ¨¡æ€æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚Qwen2.5-Omniåœ¨éŸ³é¢‘èƒ½åŠ›ä¸Šä¼˜äºç±»ä¼¼å¤§å°çš„Qwen2-Audioï¼Œå¹¶ä¸Qwen2.5-VL-7Bä¿æŒåŒç­‰æ°´å¹³ã€‚

* **å“è¶Šçš„ç«¯åˆ°ç«¯è¯­éŸ³æŒ‡ä»¤è·Ÿéšèƒ½åŠ›**ï¼šQwen2.5-Omniåœ¨ç«¯åˆ°ç«¯è¯­éŸ³æŒ‡ä»¤è·Ÿéšæ–¹é¢è¡¨ç°å‡ºä¸æ–‡æœ¬è¾“å…¥å¤„ç†ç›¸åª²ç¾çš„æ•ˆæœï¼Œåœ¨MMLUé€šç”¨çŸ¥è¯†ç†è§£å’ŒGSM8Kæ•°å­¦æ¨ç†ç­‰åŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

### æ¨¡å‹ç»“æ„

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/overview.png" width="80%"/>
<p>

### æ€§èƒ½æŒ‡æ ‡

Qwen2.5-Omniåœ¨åŒ…æ‹¬å›¾åƒï¼ŒéŸ³é¢‘ï¼ŒéŸ³è§†é¢‘ç­‰å„ç§æ¨¡æ€ä¸‹çš„è¡¨ç°éƒ½ä¼˜äºç±»ä¼¼å¤§å°çš„å•æ¨¡æ€æ¨¡å‹ä»¥åŠå°é—­æºæ¨¡å‹ï¼Œä¾‹å¦‚Qwen2.5-VL-7Bã€Qwen2-Audioå’ŒGemini-1.5-proã€‚åœ¨å¤šæ¨¡æ€ä»»åŠ¡OmniBenchï¼ŒQwen2.5-Omniè¾¾åˆ°äº†SOTAçš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œåœ¨å•æ¨¡æ€ä»»åŠ¡ä¸­ï¼ŒQwen2.5-Omniåœ¨å¤šä¸ªé¢†åŸŸä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬è¯­éŸ³è¯†åˆ«ï¼ˆCommon Voiceï¼‰ã€ç¿»è¯‘ï¼ˆCoVoST2ï¼‰ã€éŸ³é¢‘ç†è§£ï¼ˆMMAUï¼‰ã€å›¾åƒæ¨ç†ï¼ˆMMMUã€MMStarï¼‰ã€è§†é¢‘ç†è§£ï¼ˆMVBenchï¼‰ä»¥åŠè¯­éŸ³ç”Ÿæˆï¼ˆSeed-tts-evalå’Œä¸»è§‚è‡ªç„¶å¬æ„Ÿï¼‰ã€‚

<p align="center">
    <img src="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/bar.png"/>
<p>

<details>
<summary>Multimodality  -> Text</summary>

<table class="tg"><thead>
  <tr>
    <th class="tg-0lax">Datasets</th>
    <th class="tg-0lax">Model</th>
    <th class="tg-0lax">Performance</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-0lax" rowspan="10">OmniBench<br>Speech | Sound Event | Music | Avg</td>
    <td class="tg-0lax">Gemini-1.5-Pro</td>
    <td class="tg-0lax">42.67%|42.26%|46.23%|42.91%</td>
  </tr>
  <tr>
    <td class="tg-0lax">MIO-Instruct</td>
    <td class="tg-0lax">36.96%|33.58%|11.32%|33.80%</td>
  </tr>
  <tr>
    <td class="tg-0lax">AnyGPT (7B)</td>
    <td class="tg-0lax">17.77%|20.75%|13.21%|18.04%</td>
  </tr>
  <tr>
    <td class="tg-0lax">video-SALMONN</td>
    <td class="tg-0lax">34.11%|31.70%|<strong>56.60%</strong>|35.64%</td>
  </tr>
  <tr>
    <td class="tg-0lax">UnifiedIO2-xlarge</td>
    <td class="tg-0lax">39.56%|36.98%|29.25%|38.00%</td>
  </tr>
  <tr>
    <td class="tg-0lax">UnifiedIO2-xxlarge</td>
    <td class="tg-0lax">34.24%|36.98%|24.53%|33.98%</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">-|-|-|40.50%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Baichuan-Omni-1.5</td>
    <td class="tg-0lax">-|-|-|42.90%</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>55.25%</strong>|<strong>60.00%</strong>|52.83%|<strong>56.13%</strong></td>
  </tr>
</tbody></table>
</details>


<details>
<summary>Audio -> Text</summary>


<table class="tg"><thead>
  <tr>
    <th class="tg-0lax">Datasets</th>
    <th class="tg-0lax">Model</th>
    <th class="tg-0lax">Performance</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-9j4x" colspan="3">ASR</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="11">Librispeech<br>dev-clean | dev other | test-clean | test-other</td>
    <td class="tg-0lax">SALMONN</td>
    <td class="tg-0lax">-|-|2.1|4.9</td>
  </tr>
  <tr>
    <td class="tg-0lax">SpeechVerse</td>
    <td class="tg-0lax">-|-|2.1|4.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Whisper-large-v3</td>
    <td class="tg-0lax">-|-|1.8|3.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Llama-3-8B</td>
    <td class="tg-0lax">-|-|-|3.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Llama-3-70B</td>
    <td class="tg-0lax">-|-|-|3.1</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-ASR-Multilingual</td>
    <td class="tg-0lax">-|-|<strong>1.6</strong>|<strong>2.8</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">-|-|1.7|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">-|-|1.7|3.9</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">1.8|4.0|2.0|4.2</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax"><strong>1.3</strong>|<strong>3.4</strong>|<strong>1.6</strong>|3.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">1.6|3.5|1.8|3.4</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="4">Common Voice 15<br>en | zh | yue | fr</td>
    <td class="tg-0lax">Whisper-large-v3</td>
    <td class="tg-0lax">9.3|12.8|10.9|10.8</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">7.9|6.3|6.4|8.5</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">8.6|6.9|<strong>5.9</strong>|9.6</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>7.6</strong>|<strong>5.2</strong>|7.3|<strong>7.5</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="7">Fleurs<br>zh | en</td>
    <td class="tg-0lax">Whisper-large-v3</td>
    <td class="tg-0lax">7.7|4.1</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-ASR-Multilingual</td>
    <td class="tg-0lax">-|<strong>3.4</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">10.8|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">4.4|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">3.0|3.8</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">7.5|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>3.0</strong>|4.1</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="5">Wenetspeech<br>test-net | test-meeting</td>
    <td class="tg-0lax">Seed-ASR-Chinese</td>
    <td class="tg-0lax"><strong>4.7|5.7</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">-|16.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">6.9|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">6.8|7.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">5.9|7.7</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="3">Voxpopuli-V1.0-en</td>
    <td class="tg-0lax">Llama-3-8B</td>
    <td class="tg-0lax">6.2</td>
  </tr>
  <tr>
    <td class="tg-0lax">Llama-3-70B</td>
    <td class="tg-0lax"><strong>5.7</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">5.8</td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">S2TT</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="8">CoVoST2<br>en-de | de-en | en-zh | zh-en</td>
    <td class="tg-0lax">SALMONN</td>
    <td class="tg-0lax">18.6|-|33.1|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">SpeechLLaMA</td>
    <td class="tg-0lax">-|27.1|-|12.3</td>
  </tr>
  <tr>
    <td class="tg-0lax">BLSP</td>
    <td class="tg-0lax">14.1|-|-|-</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">-|-|<strong>48.2</strong>|27.2</td>
  </tr>
  <tr>
    <td class="tg-0lax">MinMo</td>
    <td class="tg-0lax">-|<strong>39.9</strong>|46.7|26.0</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">25.1|33.9|41.5|15.7</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">29.9|35.2|45.2|24.4</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>30.2</strong>|37.7|41.4|<strong>29.4</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">SER</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="5">Meld</td>
    <td class="tg-0lax">WavLM-large</td>
    <td class="tg-0lax">0.542</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">0.524</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">0.557</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">0.553</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.570</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">VSC</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="5">VocalSound</td>
    <td class="tg-0lax">CLAP</td>
    <td class="tg-0lax">0.495</td>
  </tr>
  <tr>
    <td class="tg-0lax">Pengi</td>
    <td class="tg-0lax">0.604</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen-Audio</td>
    <td class="tg-0lax">0.929</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax"><strong>0.939</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.939</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Music</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="2">GiantSteps Tempo</td>
    <td class="tg-0lax">Llark-7B</td>
    <td class="tg-0lax">0.86</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.88</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="2">MusicCaps</td>
    <td class="tg-0lax">LP-MusicCaps</td>
    <td class="tg-0lax">0.291|0.149|0.089|<strong>0.061</strong>|<strong>0.129</strong>|0.130</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>0.328</strong>|<strong>0.162</strong>|<strong>0.090</strong>|0.055|0.127|<strong>0.225</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Audio Reasoning</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="3">MMAU<br>Sound | Music | Speech | Avg</td>
    <td class="tg-0lax">Gemini-Pro-V1.5</td>
    <td class="tg-0lax">56.75|49.40|58.55|54.90</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">54.95|50.98|42.04|49.20</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>67.87|69.16|59.76|65.60</strong></td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Voice Chatting</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="8">VoiceBench<br>AlpacaEval | CommonEval | SD-QA | MMSU</td>
    <td class="tg-0lax">Ultravox-v0.4.1-LLaMA-3.1-8B</td>
    <td class="tg-0lax"><strong>4.55</strong>|3.90|53.35|47.17</td>
  </tr>
  <tr>
    <td class="tg-0lax">MERaLiON</td>
    <td class="tg-0lax">4.50|3.77|55.06|34.95</td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">3.50|2.95|25.95|27.03</td>
  </tr>
  <tr>
    <td class="tg-0lax">Lyra-Base</td>
    <td class="tg-0lax">3.85|3.50|38.25|49.74</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">4.42|<strong>4.15</strong>|50.72|54.78</td>
  </tr>
  <tr>
    <td class="tg-0lax">Baichuan-Omni-1.5</td>
    <td class="tg-0lax">4.50|4.05|43.40|57.25</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">3.74|3.43|35.71|35.72</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax">4.49|3.93|<strong>55.71</strong>|<strong>61.32</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="8">VoiceBench<br>OpenBookQA | IFEval | AdvBench | Avg</td>
    <td class="tg-0lax">Ultravox-v0.4.1-LLaMA-3.1-8B</td>
    <td class="tg-0lax">65.27|<strong>66.88</strong>|98.46|71.45</td>
  </tr>
  <tr>
    <td class="tg-0lax">MERaLiON</td>
    <td class="tg-0lax">27.23|62.93|94.81|62.91</td>
  </tr>
  <tr>
    <td class="tg-0lax">Megrez-3B-Omni</td>
    <td class="tg-0lax">28.35|25.71|87.69|46.25</td>
  </tr>
  <tr>
    <td class="tg-0lax">Lyra-Base</td>
    <td class="tg-0lax">72.75|36.28|59.62|57.66</td>
  </tr>
  <tr>
    <td class="tg-0lax">MiniCPM-o</td>
    <td class="tg-0lax">78.02|49.25|97.69|71.69</td>
  </tr>
  <tr>
    <td class="tg-0lax">Baichuan-Omni-1.5</td>
    <td class="tg-0lax">74.51|54.54|97.31|71.14</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2-Audio</td>
    <td class="tg-0lax">49.45|26.33|96.73|55.35</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B</td>
    <td class="tg-0lax"><strong>81.10</strong>|52.87|<strong>99.42</strong>|<strong>74.12</strong></td>
  </tr>
</tbody></table>
</details>

<details>
<summary>Image -> Text</summary>

| Dataset                        | Qwen2.5-Omni-7B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | 
|--------------------------------|--------------|------------|---------------|-------------|
| MMMU<sub>val</sub>             | 59.2         | 53.9       | 58.6          | **60.0**    | 
| MMMU-Pro<sub>overall</sub>     | 36.6         | -          | **38.3**      | 37.6        | 
| MathVista<sub>testmini</sub>   | 67.9         | **71.9**   | 68.2          | 52.5        | 
| MathVision<sub>full</sub>      | 25.0         | 23.1       | **25.1**      | -           | 
| MMBench-V1.1-EN<sub>test</sub> | 81.8         | 80.5       | **82.6**      | 76.0        | 
| MMVet<sub>turbo</sub>          | 66.8         | **67.5**   | 67.1          | 66.9        | 
| MMStar                         | **64.0**     | **64.0**   | 63.9          | 54.8        | 
| MME<sub>sum</sub>              | 2340         | **2372**   | 2347          | 2003        | 
| MuirBench                      | 59.2         | -          | **59.2**      | -           | 
| CRPE<sub>relation</sub>        | **76.5**     | -          | 76.4          | -           | 
| RealWorldQA<sub>avg</sub>      | 70.3         | **71.9**   | 68.5          | -           | 
| MME-RealWorld<sub>en</sub>     | **61.6**     | -          | 57.4          | -           | 
| MM-MT-Bench                    | 6.0          | -          | **6.3**       | -           | 
| AI2D                           | 83.2         | **85.8**   | 83.9          | -           | 
| TextVQA<sub>val</sub>          | 84.4         | 83.2       | **84.9**      | -           | 
| DocVQA<sub>test</sub>          | 95.2         | 93.5       | **95.7**      | -           | 
| ChartQA<sub>test Avg</sub>     | 85.3         | 84.9       | **87.3**      | -           | 
| OCRBench_V2<sub>en</sub>       | **57.8**     | -          | 56.3          | -           | 


| Dataset                  | Qwen2.5-Omni-7B | Qwen2.5-VL-7B | Grounding DINO | Gemini 1.5 Pro | 
|--------------------------|--------------|---------------|----------------|----------------|
| Refcoco<sub>val</sub>    | 90.5         | 90.0          | **90.6**       | 73.2           | 
| Refcoco<sub>textA</sub>  | **93.5**     | 92.5          | 93.2           | 72.9           | 
| Refcoco<sub>textB</sub>  | 86.6         | 85.4          | **88.2**       | 74.6           | 
| Refcoco+<sub>val</sub>   | 85.4         | 84.2          | **88.2**       | 62.5           | 
| Refcoco+<sub>textA</sub> | **91.0**     | 89.1          | 89.0           | 63.9           | 
| Refcoco+<sub>textB</sub> | **79.3**     | 76.9          | 75.9           | 65.0           | 
| Refcocog+<sub>val</sub>  | **87.4**     | 87.2          | 86.1           | 75.2           | 
| Refcocog+<sub>test</sub> | **87.9**     | 87.2          | 87.0           | 76.2           | 
| ODinW                    | 42.4         | 37.3          | **55.0**       | 36.7           | 
| PointGrounding           | 66.5         | **67.3**      | -              | -              | 
</details>


<details>
<summary>Video(without audio) -> Text</summary>

| Dataset                     | Qwen2.5-Omni-7B | Other Best | Qwen2.5-VL-7B | GPT-4o-mini | 
|-----------------------------|--------------|------------|---------------|-------------|
| Video-MME<sub>w/o sub</sub> | 64.3         | 63.9       | **65.1**      | 64.8        | 
| Video-MME<sub>w sub</sub>   | **72.4**     | 67.9       | 71.6          | -           | 
| MVBench                     | **70.3**     | 67.2       | 69.6          | -           | 
| EgoSchema<sub>test</sub>    | **68.6**     | 63.2       | 65.0          | -           | 
</details>

<details>
<summary>Zero-shot Speech Generation</summary>


<table class="tg"><thead>
  <tr>
    <th class="tg-0lax">Datasets</th>
    <th class="tg-0lax">Model</th>
    <th class="tg-0lax">Performance</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-9j4x" colspan="3">Content Consistency</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="9">SEED<br>test-zh | test-en | test-hard </td>
    <td class="tg-0lax">Seed-TTS_ICL</td>
    <td class="tg-0lax">1.11 | 2.24 | 7.58</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-TTS_RL</td>
    <td class="tg-0lax"><strong>1.00</strong> | 1.94 | <strong>6.42</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">MaskGCT</td>
    <td class="tg-0lax">2.27 | 2.62 | 10.27</td>
  </tr>
  <tr>
    <td class="tg-0lax">E2_TTS</td>
    <td class="tg-0lax">1.97 | 2.19 | -</td>
  </tr>
  <tr>
    <td class="tg-0lax">F5-TTS</td>
    <td class="tg-0lax">1.56 | <strong>1.83</strong> | 8.67</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2</td>
    <td class="tg-0lax">1.45 | 2.57 | 6.83</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2-S</td>
    <td class="tg-0lax">1.45 | 2.38 | 8.08</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_ICL</td>
    <td class="tg-0lax">1.70 | 2.72 | 7.97</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_RL</td>
    <td class="tg-0lax">1.42 | 2.32 | 6.54</td>
  </tr>
  <tr>
    <td class="tg-9j4x" colspan="3">Speaker Similarity</td>
  </tr>
  <tr>
    <td class="tg-0lax" rowspan="9">SEED<br>test-zh | test-en | test-hard </td>
    <td class="tg-0lax">Seed-TTS_ICL</td>
    <td class="tg-0lax">0.796 | 0.762 | 0.776</td>
  </tr>
  <tr>
    <td class="tg-0lax">Seed-TTS_RL</td>
    <td class="tg-0lax"><strong>0.801</strong> | <strong>0.766</strong> | <strong>0.782</strong></td>
  </tr>
  <tr>
    <td class="tg-0lax">MaskGCT</td>
    <td class="tg-0lax">0.774 | 0.714 | 0.748</td>
  </tr>
  <tr>
    <td class="tg-0lax">E2_TTS</td>
    <td class="tg-0lax">0.730 | 0.710 | -</td>
  </tr>
  <tr>
    <td class="tg-0lax">F5-TTS</td>
    <td class="tg-0lax">0.741 | 0.647 | 0.713</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2</td>
    <td class="tg-0lax">0.748 | 0.652 | 0.724</td>
  </tr>
  <tr>
    <td class="tg-0lax">CosyVoice 2-S</td>
    <td class="tg-0lax">0.753 | 0.654 | 0.732</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_ICL</td>
    <td class="tg-0lax">0.752 | 0.632 | 0.747</td>
  </tr>
  <tr>
    <td class="tg-0lax">Qwen2.5-Omni-7B_RL</td>
    <td class="tg-0lax">0.754 | 0.641 | 0.752</td>
  </tr>
</tbody></table>
</details>

<details>
<summary>Text -> Text</summary>

| Dataset                           | Qwen2.5-Omni-7B | Qwen2.5-7B | Qwen2-7B | Llama3.1-8B | Gemma2-9B | 
|-----------------------------------|-----------|------------|----------|-------------|-----------|
| MMLU-Pro                          | 47.0      | **56.3**   | 44.1     | 48.3        | 52.1      | 
| MMLU-redux                        | 71.0      | **75.4**   | 67.3     | 67.2        | 72.8      | 
| LiveBench<sub>0831</sub>          | 29.6      | **35.9**   | 29.2     | 26.7        | 30.6      | 
| GPQA                              | 30.8      | **36.4**   | 34.3     | 32.8        | 32.8      | 
| MATH                              | 71.5      | **75.5**   | 52.9     | 51.9        | 44.3      | 
| GSM8K                             | 88.7      | **91.6**   | 85.7     | 84.5        | 76.7      | 
| HumanEval                         | 78.7      | **84.8**   | 79.9     | 72.6        | 68.9      | 
| MBPP                              | 73.2      | **79.2**   | 67.2     | 69.6        | 74.9      | 
| MultiPL-E                         | 65.8      | **70.4**   | 59.1     | 50.7        | 53.4      | 
| LiveCodeBench<sub>2305-2409</sub> | 24.6      | **28.7**   | 23.9     | 8.3         | 18.9      | 
</details>

## å¿«é€Ÿå¼€å§‹

æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†æä¾›å¦‚ä½•åœ¨ğŸ¤– ModelScopeå’ŒğŸ¤— Transformersä¸Šä½¿ç”¨ Qwen2.5-Omni. ç”±äºQwen2.5-Omniçš„ä»£ç åœ¨Hugging Face transformersä¸­ç›®å‰å¤„äºæœªåˆå¹¶é˜¶æ®µï¼Œå°šæœªå¹¶å…¥ä¸»åˆ†æ”¯ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨ä»æºä»£ç æ„å»ºï¼š
```
pip uninstall transformers
pip install git+https://github.com/huggingface/transformers@f742a644ca32e65758c3adb36225aef1731bd2a8
pip install accelerate
```
å¦åˆ™æ‚¨å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é”™è¯¯ï¼š
```
KeyError: 'qwen2_5_omni'
```
æˆ–è€…æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æˆ‘ä»¬çš„[å®˜æ–¹ docker é•œåƒ](#-docker)æ¥å…å»ä»æºç æ„å»ºã€‚

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé¢å¤–çš„å°å·¥å…·ï¼Œå®ƒä½¿æ‚¨å¯ä»¥æ›´æ–¹ä¾¿åœ°åƒä½¿ç”¨APIä¸€æ ·å¤„ç†å„ç§éŸ³é¢‘å’Œè§†è§‰æ¨¡æ€çš„è¾“å…¥ï¼Œå¤„ç†æ‚¨è¾“å…¥ä¸­åŒ…æ‹¬base64ã€URLä»¥åŠäº¤é”™çš„éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘çš„æƒ…å†µã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…æ­¤å·¥å…·åŒ…ï¼Œå¹¶ç¡®ä¿æ‚¨çš„ç³»ç»Ÿå®‰è£…äº†`ffmpeg`ï¼š

```bash
# éå¸¸å»ºè®®ä½¿ç”¨`[decord]`ç‰¹æ€§æ¥è·å–æ›´å¿«çš„è§†é¢‘è¯»å–é€Ÿåº¦
pip install qwen-omni-utils[decord]
```

å¦‚æœæ‚¨æœªä½¿ç”¨Linuxæ“ä½œç³»ç»Ÿï¼Œæ‚¨å¯èƒ½æ— æ³•ä»PyPIå®‰è£…`decord`ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`pip install qwen-omni-utils`ï¼Œå®ƒå°†å›é€€åˆ°ä½¿ç”¨torchvisionè¿›è¡Œè§†é¢‘å¤„ç†ã€‚ ä½†æ˜¯ï¼Œæ‚¨ä»ç„¶å¯ä»¥[ä»æºä»£ç å®‰è£…decord](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source)ï¼Œä»¥åœ¨åŠ è½½è§†é¢‘æ—¶ä½¿ç”¨decordã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å‡†å¤‡äº†ä¸€äº› [cookbooks](https://github.com/QwenLM/Qwen2.5-Omni/tree/main/cookbooks) æ¥è¿›è¡Œèƒ½åŠ›å±•ç¤º, åŒ…æ‹¬éŸ³é¢‘ç†è§£ã€è¯­éŸ³å¯¹è¯ã€å±å¹•å½•åˆ¶äº¤äº’ã€è§†é¢‘ä¿¡æ¯æå–ã€å¤šæ¨¡æ€å¯¹è¯ç­‰ç­‰ï¼Œæ•¬è¯·è®¿é—®ã€‚

### ğŸ¤—  Transformers ä½¿ç”¨æ–¹æ³•

åœ¨è¿™é‡Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªç®€å•çš„ä»£ç ç‰‡æ®µæ¥å‘æ‚¨å±•ç¤ºå¦‚ä½•é€šè¿‡`transformers` and `qwen_omni_utils`æ¥ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹:

```python
import soundfile as sf

from transformers import Qwen2_5OmniModel, Qwen2_5OmniProcessor
from qwen_omni_utils import process_mm_info

# default: Load the model on the available device(s)
model = Qwen2_5OmniModel.from_pretrained("Qwen/Qwen2.5-Omni-7B", torch_dtype="auto", device_map="auto")

# æˆ‘ä»¬å»ºè®®å¯ç”¨ flash_attention_2 ä»¥è·å–æ›´å¿«çš„æ¨ç†é€Ÿåº¦ä»¥åŠæ›´ä½çš„æ˜¾å­˜å ç”¨.
# model = Qwen2_5OmniModel.from_pretrained(
#     "Qwen/Qwen2.5-Omni-7B",
#     torch_dtype="auto",
#     device_map="auto",
#     attn_implementation="flash_attention_2",
# )

processor = Qwen2_5OmniProcessor.from_pretrained("Qwen/Qwen2.5-Omni-7B")

conversation = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4"},
        ],
    },
]

# set use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for inference
text = processor.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversation, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors="pt", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Inference: Generation of the output text and audio
text_ids, audio = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO)

text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(text)
sf.write(
    "output.wav",
    audio.reshape(-1).detach().cpu().numpy(),
    samplerate=24000,
)
```

<details>
<summary>æœ€å°GPUå†…å­˜éœ€æ±‚</summary>

| ç²¾åº¦ | 15(s) éŸ³é¢‘ | 30(s) éŸ³é¢‘ | 60(s) éŸ³é¢‘ |
|-----------| ------------- | --------- | -------------- |
| FP32      | 93.56 GB      | ä¸æ¨è | ä¸æ¨è      |
| BF16      | 31.11 GB      | 41.85 GB  | 60.19 GB       |

æ³¨æ„: ä¸Šè¿°çš„è¡¨æ ¼æ‰€å±•ç¤ºçš„åªæ˜¯ä½¿ç”¨`transformers`è¿›è¡Œæ¨ç†çš„ç†è®ºæœ€å°å€¼ï¼Œå¹¶ä¸”`BF16`çš„ç»“æœæ˜¯åœ¨`attn_implementation="flash_attention_2"`çš„æƒ…å†µä¸‹æµ‹è¯•å¾—åˆ°çš„ï¼Œä½†æ˜¯åœ¨å®é™…ä¸­ï¼Œå†…å­˜ä½¿ç”¨é€šå¸¸æ¯”è¿™ä¸ªå€¼è¦é«˜è‡³å°‘1.2å€ã€‚ æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…[è¿™é‡Œ](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator)ã€‚
</details>  

<details>
<summary>è§†é¢‘URLè¶…é“¾æ¥ä½¿ç”¨æ–¹æ³•</summary>

è§†é¢‘URLè¶…é“¾æ¥çš„å…¼å®¹æ€§å–å†³äºç¬¬ä¸‰æ–¹åº“çš„ç‰ˆæœ¬ã€‚å…·ä½“çš„ç»†èŠ‚åœ¨ä¸‹è¡¨ä¸­æ‰€ç¤ºï¼Œå¦‚æœæ‚¨ä¸å¸Œæœ›ä½¿ç”¨é»˜è®¤å€¼ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®`FORCE_QWENVL_VIDEO_READER=torchvision`æˆ–`FORCE_QWENVL_VIDEO_READER=decord`æ¥å®ç°ã€‚

| åç«¯ç±»å‹     | HTTP | HTTPS |
|-------------|------|-------|
| torchvision >= 0.19.0 | âœ…  | âœ…   |
| torchvision < 0.19.0  | âŒ  | âŒ   |
| decord      | âœ…  | âŒ   |
</details>

<details>
<summary>æ‰¹å¤„ç†</summary>

å½“`return_audio=False`è®¾ç½®æ—¶ï¼Œæ¨¡å‹æ”¯æŒæ··åˆè¾“å…¥çš„æ‰¹å¤„ç†ï¼Œå…¶ä¸­å¯ä»¥åŒ…å«å„ç§ç±»å‹çš„æ ·æœ¬ï¼Œå¦‚æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ªä»£ç ç‰‡æ®µçš„ç¤ºä¾‹ã€‚

```python
# Sample messages for batch inference

# Conversation with video only
conversation1 = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "video": "/path/to/video.mp4"},
        ]
    }
]

# Conversation with audio only
conversation2 = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": [
            {"type": "audio", "audio": "/path/to/audio.wav"},
        ]
    }
]

# Conversation with pure text
conversation3 = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": "who are you?"
    }
]


# Conversation with mixed media
conversation4 = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "image": "/path/to/image.jpg"},
            {"type": "video", "video": "/path/to/video.mp4"},
            {"type": "audio", "audio": "/path/to/audio.wav"},
            {"type": "text", "text": "What are the elements can you see and hear in these medias?"},
        ],
    }
]

# Combine messages for batch processing
conversations = [conversation1, conversation2, conversation3, conversation4]

# set use audio in video
USE_AUDIO_IN_VIDEO = True

# Preparation for batch inference
text = processor.apply_chat_template(conversations, add_generation_prompt=True, tokenize=False)
audios, images, videos = process_mm_info(conversations, use_audio_in_video=USE_AUDIO_IN_VIDEO)

inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors="pt", padding=True, use_audio_in_video=USE_AUDIO_IN_VIDEO)
inputs = inputs.to(model.device).to(model.dtype)

# Batch Inference
text_ids = model.generate(**inputs, use_audio_in_video=USE_AUDIO_IN_VIDEO, return_audio=False)
text = processor.batch_decode(text_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
print(text)
```
</details>


### ğŸ¤– ModelScope ä½¿ç”¨æ–¹æ³•
æˆ‘ä»¬å¼ºçƒˆå»ºè®®ä¸­å›½ç”¨æˆ·ä½¿ç”¨ModelScopeæ¥è·å–æ¨¡å‹ï¼Œ`snapshot_download`å¯ä»¥è§£å†³ä¸‹è½½è¿‡ç¨‹ä¸­çš„å„ç§ç½‘ç»œé—®é¢˜ï¼Œè¯¦æƒ…è¯·å‚è€ƒ[ModelScope](https://modelscope.cn/organization/qwen)ã€‚


### ä½¿ç”¨æç¤º

#### éŸ³é¢‘è¾“å‡ºçš„æç¤ºè¯
å¦‚æœç”¨æˆ·éœ€è¦éŸ³é¢‘è¾“å‡ºï¼Œé‚£ä¹ˆç³»ç»Ÿæç¤ºå¿…é¡»è®¾ç½®ä¸º"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech."ï¼Œå¦åˆ™éŸ³é¢‘è¾“å‡ºå¯èƒ½ä¸ä¼šæŒ‰ç…§é¢„æœŸå·¥ä½œã€‚
```
{
    "role": "system",
    "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
}
```
#### ä½¿ç”¨è§†é¢‘ä¸­çš„éŸ³é¢‘
åœ¨å¤šæ¨¡æ€äº¤äº’è¿‡ç¨‹ä¸­ï¼Œç”¨æˆ·æä¾›çš„è§†é¢‘é€šå¸¸ä¼´éšç€éŸ³é¢‘ï¼ˆå¦‚å¯¹è§†é¢‘å†…å®¹çš„æé—®ï¼Œæˆ–è€…è§†é¢‘ä¸­æŸäº›äº‹ä»¶çš„å£°éŸ³ï¼‰ï¼Œè¿™äº›ä¿¡æ¯å¯¹æ¨¡å‹çš„äº¤äº’ä½“éªŒå¾ˆå…³é”®ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æä¾›äº†ä»¥ä¸‹é€‰é¡¹è®©ç”¨æˆ·å†³å®šæ˜¯å¦ä½¿ç”¨è§†é¢‘ä¸­çš„éŸ³é¢‘ã€‚
```python
# ç¬¬ä¸€å¤„ï¼Œåœ¨æ•°æ®é¢„å¤„ç†ä¸­
audios, images, videos = process_mm_info(conversations, use_audio_in_video=True)
```
```python
# ç¬¬äºŒå¤„ï¼Œåœ¨æ¨¡å‹å¤„ç†ä¸­
inputs = processor(text=text, audios=audios, images=images, videos=videos, return_tensors="pt", 
                   padding=True, use_audio_in_video=True)
```
```python
# ç¬¬ä¸‰å¤„ï¼Œåœ¨æ¨¡å‹æ¨ç†ä¸­
text_ids, audio = model.generate(**inputs, use_audio_in_video=True)
```
å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¤šè½®å¯¹è¯è¿‡ç¨‹ä¸­ï¼Œ`use_audio_in_video`å‚æ•°åœ¨è¿™å‡ ä¸ªåœ°æ–¹å¿…é¡»è®¾ç½®ä¸ºç›¸åŒçš„å€¼ï¼Œå¦åˆ™å¯èƒ½ä¼šå‡ºç°éé¢„æœŸçš„ç»“æœã€‚

#### æ˜¯å¦ä½¿ç”¨éŸ³é¢‘è¾“å‡º
æ¨¡å‹æ”¯æŒæ–‡æœ¬å’ŒéŸ³é¢‘è¾“å‡ºï¼Œå¦‚æœç”¨æˆ·ä¸éœ€è¦éŸ³é¢‘è¾“å‡ºï¼Œå¯ä»¥åœ¨`from_pretrained`å‡½æ•°ä¸­è®¾ç½®`enable_audio_output=False`ï¼Œæ­¤é€‰é¡¹å°†èŠ‚çœçº¦`~2GB`çš„GPUå†…å­˜ï¼Œä½†`generate`å‡½æ•°çš„`return_audio`é€‰é¡¹å°†åªèƒ½è®¾ç½®ä¸º`False`ã€‚

```python
model = Qwen2_5OmniModel.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto",
    enable_audio_output=False,
)
```

ä¸ºäº†è·å¾—çµæ´»çš„ä½“éªŒï¼Œæˆ‘ä»¬å»ºè®®åœ¨é€šè¿‡`from_pretrained`å‡½æ•°åˆå§‹åŒ–æ¨¡å‹æ—¶è®¾ç½®`enable_audio_output`ä¸º`True`ï¼Œç„¶ååœ¨è°ƒç”¨`generate`å‡½æ•°æ—¶æ ¹æ®éœ€è¦å†³å®šæ˜¯å¦è¿”å›éŸ³é¢‘ã€‚å½“`return_audio`è®¾ç½®ä¸º`False`æ—¶ï¼Œæ¨¡å‹å°†ä»…è¿”å›æ–‡æœ¬è¾“å‡ºä»¥æ›´å¿«åœ°è·å–æ–‡æœ¬å“åº”ã€‚

```python
model = Qwen2_5OmniModel.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    torch_dtype="auto",
    device_map="auto",
    enable_audio_output=True,
)
...
text_ids = model.generate(**inputs, return_audio=False)
```

#### ä¿®æ”¹è¾“å‡ºè¯­éŸ³çš„éŸ³è‰²ç±»å‹
Qwen2.5-Omni æ”¯æŒä¿®æ”¹è¾“å‡ºè¯­éŸ³çš„éŸ³è‰²ç±»å‹ï¼Œ`"Qwen/Qwen2.5-Omni-7B"`ç›®å‰æ”¯æŒä¸¤ç§å¦‚ä¸‹ä¸¤ç§éŸ³è‰²ç±»å‹ï¼š


| éŸ³è‰²ç±»å‹ | æ€§åˆ« | æè¿° |
|------------|--------|-------------|
| Chelsie    | å¥³ | ç”œç¾ï¼Œæ¸©å©‰ï¼Œæ˜äº®ï¼Œè½»æŸ”|
| Ethan      | ç”·   | é˜³å…‰ï¼Œæ´»åŠ›ï¼Œè½»å¿«ï¼Œäº²å’Œ|

ç”¨æˆ·å¯ä»¥ä½¿ç”¨`generate`å‡½æ•°çš„`spk`å‚æ•°æ¥æŒ‡å®šéŸ³è‰²ç±»å‹ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®š`spk`ï¼Œåˆ™é»˜è®¤éŸ³è‰²ç±»å‹ä¸º`Chelsie`ã€‚

```python
text_ids, audio = model.generate(**inputs, spk="Chelsie")
```

```python
text_ids, audio = model.generate(**inputs, spk="Ethan")
```

#### ä½¿ç”¨Flash-Attention 2åŠ é€Ÿç”Ÿæˆ

é¦–å…ˆï¼Œè¯·ç¡®ä¿å·²å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„Flash Attention 2ï¼š

```bash
pip install -U flash-attn --no-build-isolation
```

å½“ç„¶ï¼Œæ‚¨è¿˜éœ€è¦ç¡®ä¿æ‚¨çš„ç¡¬ä»¶å…¼å®¹Flash Attention 2ï¼Œå¯ä»¥ä»[å®˜æ–¹æ–‡æ¡£](https://github.com/Dao-AILab/flash-attention)æ¥è·å–æ›´å¤šä¿¡æ¯ã€‚FlashAttention-2ä»…å¯åœ¨æ¨¡å‹åŠ è½½åˆ°`torch.float16`æˆ–`torch.bfloat16`æ—¶ä½¿ç”¨ã€‚

ä¸ºäº†åŠ è½½å¹¶è¿è¡Œä½¿ç”¨FlashAttention-2çš„æ¨¡å‹ï¼Œåœ¨åŠ è½½æ¨¡å‹æ—¶æ·»åŠ `attn_implementation="flash_attention_2"`ï¼š

```python
from transformers import Qwen2_5OmniModel

model = Qwen2_5OmniModel.from_pretrained(
    "Qwen/Qwen2.5-Omni-7B",
    device_map="auto",
    torch_dtype=torch.bfloat16,
    attn_implementation="flash_attention_2",
)
```


### æ›´å¤šä½¿ç”¨ç”¨ä¾‹çš„ Cookbooks

| Cookbook | æè¿° | æ‰“å¼€ |
| -------- | ----------- | ---- |
| [é€šç”¨è¯­éŸ³ç†è§£](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/universal_audio_understanding.ipynb) | è¯­éŸ³è¯†åˆ«ï¼Œè¯­éŸ³åˆ°æ–‡æœ¬ç¿»è¯‘ï¼ŒéŸ³é¢‘åˆ†æã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/universal_audio_understanding.ipynb) |
| [è¯­éŸ³å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb) | å’Œ Qwen2.5-Omin é€šè¿‡è¯­éŸ³è¾“å…¥å’Œè¾“å‡ºå¯¹è¯ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/voice_chatting.ipynb) |
| [å±å¹•å½•åˆ¶äº¤äº’](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/screen_recording_interaction.ipynb) | ä»æ­£åœ¨å®æ—¶å½•åˆ¶çš„å±å¹•ä¸­é€šè¿‡æé—®çš„æ–¹å¼è·å–æƒ³äº†è§£çš„ä¿¡æ¯å’Œå†…å®¹ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/screen_recording_interaction.ipynb) |
| [è§†é¢‘ä¿¡æ¯æå–](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/video_information_extracting.ipynb) | ä»è§†é¢‘æµä¸­è·å–ä¿¡æ¯ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/video_information_extracting.ipynb) |
| [å…³äºéŸ³ä¹çš„å…¨æ¨¡æ€å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_music.ipynb) | å’Œ Qwen2.5-Omni é€šè¿‡éŸ³è§†é¢‘æµçš„äº¤äº’æ–¹å¼èŠèŠå…³äºéŸ³ä¹çš„è¯é¢˜ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_music.ipynb) |
| [å…³äºæ•°å­¦çš„å…¨æ¨¡æ€å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_math.ipynb) | å’Œ Qwen2.5-Omni é€šè¿‡éŸ³è§†é¢‘æµçš„äº¤äº’æ–¹å¼èŠèŠå…³äºæ•°å­¦çš„è¯é¢˜ã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/omni_chatting_for_math.ipynb) |
| [å¤šè½®å…¨æ¨¡æ€å¯¹è¯](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb) |  ä¸ Qwen2.5-Omni è¿›è¡Œäº†å¤šè½®å…¨æ¨¡æ€çš„éŸ³è§†é¢‘å¯¹è¯ï¼Œæä¾›æœ€å…¨é¢çš„èƒ½åŠ›æ¼”ç¤ºã€‚ | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/QwenLM/Qwen2.5-Omni/blob/main/cookbooks/multi_round_omni_chatting.ipynb) |

### API æ¨ç†

ä¸ºäº†è¿‘ä¸€æ­¥æ¢ç´¢ Qwen2.5-Omniï¼Œæˆ‘ä»¬é¼“åŠ±æ‚¨ä½¿ç”¨æˆ‘ä»¬çš„æœ€æ–° API æœåŠ¡æ¥è·å–æ›´å¿«æ›´é«˜æ•ˆçš„ä½“éªŒã€‚

#### å®‰è£…
```bash
pip install openai
```

#### ç¤ºä¾‹
æ‚¨å¯ä»¥æŒ‰ç…§å¦‚ä¸‹çš„ç¤ºä¾‹ä½¿ç”¨ OpenAI API æœåŠ¡ä¸ Qwen2.5-Omni è¿›è¡Œäº¤äº’ï¼Œå¯¹äºæ›´å¤šçš„ä½¿ç”¨æ–¹æ³•ï¼Œè¯·å‚è€ƒé˜¿é‡Œäº‘çš„[æ•™ç¨‹](https://help.aliyun.com/zh/model-studio/user-guide/qwen-omni)ã€‚
```python
import base64
import numpy as np
import soundfile as sf

from openai import OpenAI

client = OpenAI(
    api_key="your_api_key",
    base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
)

messages = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": [
            {"type": "video_url", "video_url": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4"},
        ],
    },
]

# Qwen-Omni only supports stream mode
completion = client.chat.completions.create(
    model="qwen-omni-turbo",
    messages=messages,
    modalities=["text", "audio"],
    audio={
        "voice": "Cherry", # Cherry, Ethan, Serena, Chelsie is available
        "format": "wav"
    },
    stream=True,
    stream_options={"include_usage": True}
)

text = []
audio_string = ""
for chunk in completion:
    if chunk.choices:
        if hasattr(chunk.choices[0].delta, "audio"):
            try:
                audio_string += chunk.choices[0].delta.audio["data"]
            except Exception as e:
                text.append(chunk.choices[0].delta.audio["transcript"])
    else:
        print(chunk.usage)

print("".join(text))
wav_bytes = base64.b64decode(audio_string)
wav_array = np.frombuffer(wav_bytes, dtype=np.int16)
sf.write("output.wav", wav_array, samplerate=24000)
```

## å’Œ Qwen2.5-Omni å¯¹è¯

### åœ¨çº¿æ¼”ç¤º

æ— éœ€éƒ¨ç½²ï¼Œæ‚¨å¯ä»¥ç›´æ¥è®¿é—®æˆ‘ä»¬çš„ [Hugginface Spaces](https://huggingface.co/spaces/Qwen/Qwen2.5-Omni-7B-Demo) å’Œ [Modelscope åˆ›ç©ºé—´](https://modelscope.cn/studios/Qwen/Qwen2.5-Omni-Demo) æ¥ç›´æ¥ä½“éªŒåœ¨çº¿ç½‘é¡µæ¼”ç¤ºã€‚

### å¯åŠ¨æœ¬åœ°ç½‘é¡µæ¼”ç¤º

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†å¦‚ä½•æ„å»ºä¸€ä¸ªåŸºäºç½‘é¡µçš„ UIï¼ˆç”¨æˆ·ç•Œé¢ï¼‰æ¼”ç¤ºçš„æŒ‡å—ï¼Œæ­¤ UI æ¼”ç¤ºå…è®¸ç”¨æˆ·é€šè¿‡æµè§ˆå™¨ä¸é¢„å®šä¹‰çš„æ¨¡å‹æˆ–åº”ç”¨ç¨‹åºè¿›è¡Œäº¤äº’ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å¼€å§‹ä½¿ç”¨æˆ–æ‚¨å¯ä»¥ç›´æ¥ä»æˆ‘ä»¬çš„[å®˜æ–¹ Docker é•œåƒ](#-docker)ä¸­å¯åŠ¨ç½‘é¡µæ¼”ç¤ºã€‚

#### å®‰è£…

åœ¨æ‚¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…å®ƒä»¬ï¼š

```bash
pip install -r requirements_web_demo.txt
```

#### åŸºäº FlashAttention-2 å¯åŠ¨æ¼”ç¤º

ä¸€æ—¦æ‚¨å·²å®‰è£…æ‰€éœ€çš„ä¾èµ–é¡¹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µæ¼”ç¤ºã€‚æ­¤å‘½ä»¤å°†å¯åŠ¨ä¸€ä¸ª Web æœåŠ¡ï¼Œå¹¶æä¾›æ‚¨ç”¨äºåœ¨ Web æµè§ˆå™¨ä¸­è®¿é—® UI çš„é“¾æ¥ã€‚

**å»ºè®®**: ä¸ºäº†è·å¾—æ›´å¥½çš„æ€§èƒ½å’Œæ•ˆç‡ï¼Œå°¤å…¶æ˜¯å¤„ç†å¤§é‡å›¾åƒå’Œè§†é¢‘çš„åœºæ™¯ä¸‹ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®ä½¿ç”¨ [FlashAttention-2](https://github.com/Dao-AILab/flash-attention)ã€‚FlashAttention-2 æä¾›äº†æ˜¾å­˜ä½¿ç”¨å’Œé€Ÿåº¦çš„æ˜¾è‘—æ”¹è¿›ï¼Œå› æ­¤å¯¹äºå¤„ç†å¤§å‹æ¨¡å‹å’Œæ•°æ®å¤„ç†çš„åœºæ™¯ï¼Œå®ƒéå¸¸åˆé€‚ã€‚

ä¸ºäº†å¯ç”¨ FlashAttention-2ï¼Œä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¯åŠ¨æ¼”ç¤ºï¼š

```bash
python web_demo.py --flash-attn2
```

è¿™å°†ä¼šåŠ è½½æ¨¡å‹å¹¶å¯ç”¨ FlashAttention-2ã€‚

**é»˜è®¤ä½¿ç”¨æ–¹æ³•**: å¦‚æœæ‚¨æ›´å¸Œæœ›åœ¨ä¸ä½¿ç”¨ FlashAttention-2 çš„æƒ…å†µä¸‹è¿è¡Œæ¼”ç¤ºï¼Œä¹Ÿå³ä¸æŒ‡å®š`--flash-attn2`å‚æ•°ï¼Œæ­¤æ—¶æ¼”ç¤ºå°†ä¼šä½¿ç”¨é»˜è®¤çš„æ³¨æ„åŠ›æœºåˆ¶å®ç°æ–¹æ³•æ¥åŠ å™ªå’Œè¿è¡Œæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```bash
python web_demo.py
```

åœ¨è¿è¡Œè¿™ä¸ªå‘½ä»¤ä¹‹å, æ‚¨å°†ä¼šåœ¨ç»ˆç«¯ä¸­çœ‹åˆ°ç±»ä¼¼çš„è¾“å‡ºï¼š

```
Running on local: http://127.0.0.1:7860/
```

å¤åˆ¶è¯¥é“¾æ¥ï¼Œå¹¶å°†å…¶ç²˜è´´åˆ°æµè§ˆå™¨ä¸­ï¼Œå³å¯è®¿é—®ç½‘é¡µæ¼”ç¤ºï¼Œåœ¨ç½‘é¡µä¸­æ‚¨å¯ä»¥è¾“å…¥æ–‡æœ¬ã€ä¸Šä¼ éŸ³é¢‘ã€å›¾åƒå’Œè§†é¢‘ï¼Œä»¥åŠåˆ‡æ¢è¾“å‡ºéŸ³è‰²ç±»å‹ç­‰åŠŸèƒ½ã€‚


### å®æ—¶äº¤äº’

å®æ—¶äº¤äº’ä½“éªŒç›®å‰å·²ç»ä¸Šçº¿ï¼Œè¯·æ‚¨è®¿é—®[Qwen Chat](https://chat.qwen.ai/)ï¼Œå¹¶åœ¨èŠå¤©æ¡†ä¸­é€‰æ‹©è¯­éŸ³/è§†é¢‘é€šè¯ï¼Œå³å¯ä½“éªŒä¸Qwen2.5-Omniçš„å®æ—¶äº¤äº’ã€‚


## ä½¿ç”¨vLLMéƒ¨ç½²

æˆ‘ä»¬å»ºè®®ä½¿ç”¨vLLMè¿›è¡ŒQwen2.5-Omniçš„å¿«é€Ÿéƒ¨ç½²å’Œæ¨ç†ï¼Œæ‚¨éœ€è¦ä»æˆ‘ä»¬æä¾›çš„[æºç ](https://github.com/fyabc/vllm/tree/qwen2_omni_public_v1)æ„å»ºvLLMä»¥è·å–å¯¹Qwen2.5-Omniæ”¯æŒï¼Œæˆ–è€…ä½¿ç”¨æˆ‘ä»¬çš„[å®˜æ–¹ docker é•œåƒ](#-docker)ã€‚æ‚¨ä¹Ÿå¯ä»¥æŸ¥çœ‹[vLLMå®˜æ–¹æ–‡æ¡£](https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html)ä»¥è·å–åœ¨çº¿æœåŠ¡å’Œç¦»çº¿æ¨ç†çš„æ›´å¤šä¿¡æ¯ã€‚

### å®‰è£…
```bash
pip install git+https://github.com/huggingface/transformers@d40f54fc2f1524458669048cb40a8d0286f5d1d2
pip install accelerate
pip install qwen-omni-utils
git clone -b qwen2_omni_public_v1 https://github.com/fyabc/vllm.git
cd vllm
pip install .
```

### æœ¬åœ°ç¦»çº¿æ¨ç†

æ‚¨å¯ä»¥ä½¿ç”¨vLLMæ¥æœ¬åœ°ç¦»çº¿æ¨ç†Qwen2.5-Omniï¼Œç›®å‰æˆ‘ä»¬åªæ”¯æŒvllmçš„thinkeréƒ¨åˆ†ï¼Œå› æ­¤è¾“å‡ºçš„æ¨¡å‹åªèƒ½æ˜¯æ–‡æœ¬ã€‚æˆ‘ä»¬å°†åœ¨ä¸ä¹…çš„æœªæ¥æ”¯æŒæ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ä»¥å®ç°éŸ³é¢‘è¾“å‡ºã€‚

```python
import os
import torch

from transformers import Qwen2_5OmniProcessor
from vllm import LLM, SamplingParams
from qwen_omni_utils import process_mm_info

# vLLM engine v1 not supported yet
os.environ['VLLM_USE_V1'] = '0'

MODEL_PATH = "Qwen/Qwen2.5-Omni-7B"

llm = LLM(
    model=MODEL_PATH, trust_remote_code=True, gpu_memory_utilization=0.9,
    tensor_parallel_size=torch.cuda.device_count(),
    limit_mm_per_prompt={'image': 1, 'video': 1, 'audio': 1},
    seed=1234,
)

sampling_params = SamplingParams(
    temperature=1e-6,
    max_tokens=512,
)

processor = Qwen2_5OmniProcessor.from_pretrained(MODEL_PATH)

messages = [
    {
        "role": "system",
        "content": "You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.",
    },
    {
        "role": "user",
        "content": [
            {"type": "video", "video": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-Omni/draw.mp4"},
        ],
    },
]

text = processor.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True,
)

audios, images, videos = process_mm_info(messages, use_audio_in_video=True)

inputs = {
    'prompt': text[0],
    'multi_modal_data': {},
    "mm_processor_kwargs": {
        "use_audio_in_video": True,
    },
}


if images is not None:
    inputs['multi_modal_data']['image'] = images
if videos is not None:
    inputs['multi_modal_data']['video'] = videos
if audios is not None:
    inputs['multi_modal_data']['audio'] = audios

outputs = llm.generate(inputs, sampling_params=sampling_params)
print(outputs[0].outputs[0].text)
```

æˆ‘ä»¬ä¹Ÿåœ¨æˆ‘ä»¬æä¾›çš„[vLLM ä»“åº“](https://github.com/fyabc/vllm/tree/qwen2_omni_public_v1/examples/offline_inference)ä¸­æä¾›äº†ä¸€äº›ç¤ºä¾‹ä»£ç :

```bash
cd vllm

# Audio + image + video
python examples/offline_inference/qwen2_5_omni/only_thinker.py -q mixed_modalities

# Read vision and audio inputs from a single video file
# NOTE: V1 engine not supported yet.
VLLM_USE_V1=0 python examples/offline_inference/qwen2_5_omni/only_thinker.py -q use_audio_in_video

# Process audio inputs
python examples/offline_inference/audio_language.py --model-type qwen2_5_omni

# Process image inputs
python examples/offline_inference/vision_language.py --modality image --model-type qwen2_5_omni

# Process video inputs
python examples/offline_inference/vision_language.py --modality video --model-type qwen2_5_omni
```

## ğŸ³ Docker

ä¸ºäº†ç®€åŒ–éƒ¨ç½²è¿‡ç¨‹ï¼Œæˆ‘ä»¬æä¾›äº†é¢„æ„å»ºç¯å¢ƒï¼š[qwenllm/qwen-omni](https://hub.docker.com/r/qwenllm/qwen-omni)ï¼Œæ‚¨åªéœ€è¦å®‰è£…é©±åŠ¨å¹¶ä¸‹è½½æ¨¡å‹æ–‡ä»¶å³å¯å¯åŠ¨æ¼”ç¤ºã€‚

```bash
docker run --gpus all --ipc=host --network=host --rm --name qwen2.5-omni -it qwenllm/qwen-omni:2.5-cu121 bash
```

å¹¶ä¸”æ‚¨ä¹Ÿå¯ä»¥ç›´æ¥é€šè¿‡å¦‚ä¸‹å‘½ä»¤å¯åŠ¨ç½‘é¡µæ¼”ç¤º:
```bash
bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B
```
å¦‚éœ€å¯ç”¨FlashAttention-2ï¼Œè¯·ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤:
```bash
bash docker/docker_web_demo.sh --checkpoint /path/to/Qwen2.5-Omni-7B --flash-attn2
```


## å¼•ç”¨

å¦‚æœæ‚¨åœ¨æ‚¨çš„ç ”ç©¶ä¸­æ„Ÿåˆ° Qwen2.5-Omni ä¸ºæ‚¨æä¾›äº†å¸®åŠ©ï¼ŒæœŸå¾…æ‚¨èƒ½ç»™ä¸€ä¸ª Star :star: å’Œå¼•ç”¨ :pencil: :)



```BibTeX

@article{Qwen2.5-Omni,
  title={Qwen2.5-Omni Technical Report},
  author={Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, Junyang Lin},
  journal={arXiv preprint arXiv:2503.20215},
  year={2025}
}
```

<br>
